{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and syncronize data \n",
    "it extact data from each trial and save it to \n",
    "    /dataset/labeledData/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "class make_folder_dataset:\n",
    "    def __init__(self, folder_path:str,save_path) -> None:\n",
    "        self.path = folder_path\n",
    "        self.save_path = save_path\n",
    "        #os.makedirs(self.save_path)\n",
    "        self.num_lines_per_message = 130\n",
    "        self.df = pd.DataFrame()\n",
    "        self.df_dataset = pd.DataFrame()\n",
    "        self.tau = ['tau_J0','tau_J1', 'tau_J2', 'tau_J3', 'tau_J4', 'tau_J5', 'tau_J6']\n",
    "        self.tau_d = ['tau_J_d0','tau_J_d1', 'tau_J_d2', 'tau_J_d3', 'tau_J_d4', 'tau_J_d5', 'tau_J_d6']\n",
    "        self.tau_ext =['tau_ext0','tau_ext1','tau_ext2','tau_ext3','tau_ext4','tau_ext5','tau_ext6']\n",
    "\n",
    "        self.q = ['q0','q1','q2','q3','q4','q5','q6']\n",
    "        self.q_d = ['q_d0','q_d1','q_d2','q_d3','q_d4','q_d5','q_d6']\n",
    "\n",
    "        self.dq = ['dq0','dq1','dq2','dq3','dq4','dq5','dq6']\n",
    "        self.dq_d = ['dq_d0','dq_d1','dq_d2','dq_d3','dq_d4','dq_d5','dq_d6']\n",
    "\n",
    "\n",
    "        self.e = ['e0','e1','e2','e3','e4','e5','e6']\n",
    "        self.de = ['de0','de1','de2','de3','de4','de5','de6']\n",
    "        self.etau = ['etau_J0','etau_J1', 'etau_J2', 'etau_J3', 'etau_J4', 'etau_J5', 'etau_J6']\n",
    "    \n",
    "    def _extract_array(self, data_dict:dict, data_frame:str, header:list,  n:int):\n",
    "            dof = 7\n",
    "            x, y = data_frame[n].split(':')\n",
    "            y = y.replace('[','')\n",
    "            y = y.replace(']','')\n",
    "            y = y.replace('\\n','')\n",
    "\n",
    "            y = y.split(',')\n",
    "            for i in range(dof):\n",
    "                data_dict[header[i]].append(float(y[i]))\n",
    "\n",
    "    def extract_robot_data(self):\n",
    "        # it extracts robot data from all_data.txt\n",
    "        f = open(self.path + 'all_data.txt', 'r')\n",
    "        lines = f.readlines()\n",
    "\n",
    "        keywords = ['time'] + self.tau + self.tau_d + self.tau_ext + self.q + self.q_d + self.dq + self.dq_d \n",
    "\n",
    "        data_dict = dict.fromkeys(keywords)\n",
    "        for i in keywords:\n",
    "            data_dict[i]=[0]\n",
    "        \n",
    "        for i in range(int(len(lines)/self.num_lines_per_message)):\n",
    "            data_frame = lines[i*self.num_lines_per_message:(i+1)*self.num_lines_per_message]\n",
    "            \n",
    "            x, y = data_frame[3].split(':')\n",
    "            time_ = int(y)-int(int(y)/1000000)*1000000\n",
    "\n",
    "            x, y = data_frame[4].split(':')\n",
    "            time_ = time_+int(y)/np.power(10,9)\n",
    "\n",
    "            data_dict['time'].append(time_)\n",
    "            \n",
    "            self._extract_array(data_dict,data_frame,self.tau, 25)\n",
    "            self._extract_array(data_dict,data_frame,self.tau_d, 26)\n",
    "            self._extract_array(data_dict,data_frame, self.tau_ext, 37)\n",
    "            \n",
    "            self._extract_array(data_dict,data_frame,self.q, 28)\n",
    "            \n",
    "            self._extract_array(data_dict,data_frame, self.q_d, 29)\n",
    "            self._extract_array(data_dict,data_frame, self.dq, 30)\n",
    "            self._extract_array(data_dict,data_frame, self.dq_d, 31)\n",
    "        \n",
    "       \n",
    "        self.df = pd.DataFrame.from_dict(data_dict)\n",
    "        self.df = self.df.drop(index=0).reset_index()\n",
    "        \n",
    "        for i in range(len(self.e)):\n",
    "            self.df[self.e[i]] = self.df[self.q_d[i]]-self.df[self.q[i]]\n",
    "        for i in range(len(self.de)):\n",
    "            self.df[self.de[i]] = self.df[self.dq_d[i]]-self.df[self.dq[i]]\n",
    "        for i in range(len(self.etau)):\n",
    "            self.df[self.etau[i]] = self.df[self.tau_d[i]]-self.df[self.tau[i]]\n",
    "\n",
    "        #self.df.to_csv(self.save_path +'robot_data.csv',index=False)\n",
    "\n",
    "    def get_labels(self):\n",
    "        # it syncronize true labeled (contact- noncontact) data with robot data\n",
    "        true_label = pd.read_csv(self.path+'true_label.csv')\n",
    "        true_label['time'] = true_label['time_sec']+true_label['time_nsec']-self.df['time'][0]\n",
    "        time_dev = true_label['time'].diff()\n",
    "        contact_events_index = np.append([0], true_label['time'][time_dev>0.05].index.values)\n",
    "        contact_events_index = np.append(contact_events_index,  true_label['time'].shape[0]-1)\n",
    "\n",
    "        self.df['time'] = self.df['time'] - self.df['time'][0]\n",
    "        contact_count = 0\n",
    "        self.df['label']=0\n",
    "\n",
    "        for i in range(self.df['time'].shape[0]):\n",
    "            if (self.df['time'][i]-true_label['time'][contact_events_index[contact_count]]) > 0:\n",
    "                #print(i ,',', contact_events_index[contact_count], ',',self.df['time'][i], '   ', true_label['time'][contact_events_index[contact_count]] )\n",
    "                contact_count += 1\n",
    "                if contact_count == len(contact_events_index):\n",
    "                    break\n",
    "                for j in range(i, self.df['time'].shape[0]):\n",
    "                    self.df.loc[j, 'label'] = 1\n",
    "                    #print(j)\n",
    "                    if (self.df['time'][j] - true_label['time'][contact_events_index[contact_count]-1]) > 0:\n",
    "                        #print(j ,',', contact_events_index[contact_count]-1, ',', self.df['time'][j], '   ', true_label['time'][contact_events_index[contact_count]-1] )\n",
    "                        #print('----------------------------------------')\n",
    "                        i = j\n",
    "                        break\n",
    "        print(self.df.head())\n",
    "        self.df.to_csv(self.save_path + 'labeled_data.csv', index=False)\n",
    "        \n",
    "    def make_sequence(self):\n",
    "        #window_time = 140ms\n",
    "        seq_num = 28\n",
    "        gap = 4\n",
    "        selected_features= self.e + self.tau\n",
    "\n",
    "        dataset = pd.DataFrame(np.ones((int((self.df.shape[0]-seq_num)/gap), seq_num*len(selected_features)+1))*2 )\n",
    "        index = 0\n",
    "        state = False\n",
    "        last_contact_indexes = self.df.loc[self.df['label']==1,'index'].values\n",
    "        last_contact_indexes = last_contact_indexes[last_contact_indexes.shape[0]-1]\n",
    "\n",
    "        for i in range(0, last_contact_indexes, gap):\n",
    "            if state: \n",
    "                window = self.df[selected_features][i:i+seq_num]\n",
    "                dataset.iloc[index,0] = self.df['label'][i+seq_num]\n",
    "                dataset.iloc[index, 1:len(dataset.columns)] = np.hstack(window.to_numpy())\n",
    "                index += 1\n",
    "            else:\n",
    "                if self.df['label'][i+seq_num] == 1:\n",
    "                    state = 1\n",
    "        self.dataset = dataset.drop(index=dataset.loc[dataset[0]==2,0].index)\n",
    "\n",
    "        name = self.path.split('/')[len(self.path.split('/'))-2]+'.csv'\n",
    "        self.dataset.to_csv(self.save_path+name, index=False)\n",
    "        return self.dataset\n",
    "    \n",
    "    def split_data(self, train_split_rate = 0.75):\n",
    "        msk = np.random.rand(len(self.dataset)) < train_split_rate\n",
    "        train = self.dataset.loc[msk, :]\n",
    "        test = self.dataset.loc[~msk, :]\n",
    "        name = self.path.split('/')[len(self.path.split('/'))-2]+'_train.csv'\n",
    "        train.to_csv(self.save_path+name, index=False)\n",
    "\n",
    "        name = self.path.split('/')[len(self.path.split('/'))-2]+'_test.csv'\n",
    "        test.to_csv(self.save_path+name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labeled_data']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../tactileGestureDetection/DATA/labeled_dataall_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#os.makedirs(save_path,exist_ok= True)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m instance \u001b[38;5;241m=\u001b[39m make_folder_dataset(folder_path,save_path)\n\u001b[0;32m---> 20\u001b[0m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_robot_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m instance\u001b[38;5;241m.\u001b[39mget_labels() \n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(instance\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m, in \u001b[0;36mmake_folder_dataset.extract_robot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_robot_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# it extracts robot data from all_data.txt\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_data.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     44\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_d \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_ext \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_d \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdq_d \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../tactileGestureDetection/DATA/labeled_dataall_data.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#data_path = '../frankaRobot/DATA/'\n",
    "data_path = '../tactileGestureDetection/DATA/'\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "robot_data_path = data_path\n",
    "label_data_path = data_path\n",
    "dataset_path = data_path+'labeled_data/'\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Get a list of all files and directories in the specified directory\n",
    "files_and_dirs = os.listdir(robot_data_path)\n",
    "print(files_and_dirs)\n",
    "\n",
    "for folder_name in files_and_dirs:\n",
    "    folder_path = robot_data_path + folder_name\n",
    "    save_path = label_data_path + folder_name\n",
    "    #os.makedirs(save_path,exist_ok= True)\n",
    "    instance = make_folder_dataset(folder_path,save_path)\n",
    "    instance.extract_robot_data()\n",
    "    instance.get_labels() \n",
    "    print(instance.df.head())\n",
    "\n",
    "\"\"\" for i in os.listdir(rawDataPath):\n",
    "    if len(i.split('.'))==1:\n",
    "        submain_path = rawDataPath+'/'+i+'/'\n",
    "        print(submain_path)\n",
    "        for j in os.listdir(submain_path):\n",
    "            \n",
    "            if len(j.split('.'))==1:\n",
    "                print(submain_path+j+'/')\n",
    "                instance = make_folder_dataset(submain_path+j+'/')\n",
    "                instance.extract_robot_data()\n",
    "                instance.get_labels() \n",
    "                print(instance.df.head()) \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "# Using plotly + cufflinks in offline mode\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['e1','tau_J1']\n",
    "\n",
    "for i in target:\n",
    "    A = instance.df[i].max()-instance.df[i].min()\n",
    "    instance.df['label_scaled']=instance.df['label']*A + instance.df[i][0] -A/2\n",
    "    instance.df.iplot(x='time', y= [i, 'label_scaled'], xTitle='time (sec)', yTitle=i)\n",
    "    \n",
    "    #plt.plot(instance.df['time'],instance.df['labeled_scaled'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_master_train[0][df_master_master_train[0]==1]\n",
    "target = ['e0','tau_J0']\n",
    "\n",
    "for i in target:\n",
    "    A = instance.df[i].max()-instance.df[i].min()\n",
    "    instance.df['label_scaled']=instance.df['label']*A + instance.df[i][0] -A/2\n",
    "    instance.df.iplot(x='time', y= [i, 'label_scaled'], xTitle='time (sec)', yTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_master_train[0][df_master_master_train[0]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = ['tau_J0','tau_J1', 'tau_J2', 'tau_J3', 'tau_J4', 'tau_J5', 'tau_J6']\n",
    "tau_d = ['tau_J_d0','tau_J_d1', 'tau_J_d2', 'tau_J_d3', 'tau_J_d4', 'tau_J_d5', 'tau_J_d6']\n",
    "tau_ext =['tau_ext0','tau_ext1','tau_ext2','tau_ext3','tau_ext4','tau_ext5','tau_ext6']\n",
    "\n",
    "q = ['q0','q1','q2','q3','q4','q5','q6']\n",
    "q_d = ['q_d0','q_d1','q_d2','q_d3','q_d4','q_d5','q_d6']\n",
    "\n",
    "dq = ['dq0','dq1','dq2','dq3','dq4','dq5','dq6']\n",
    "dq_d = ['dq_d0','dq_d1','dq_d2','dq_d3','dq_d4','dq_d5','dq_d6']\n",
    "\n",
    "\n",
    "e = ['e0','e1','e2','e3','e4','e5','e6']\n",
    "de = ['de0','de1','de2','de3','de4','de5','de6']\n",
    "etau = ['etau_J0','etau_J1', 'etau_J2', 'etau_J3', 'etau_J4', 'etau_J5', 'etau_J6']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, train_split_rate = 0.75):\n",
    "    msk = np.random.rand(len(dataset)) < train_split_rate\n",
    "    train = dataset.loc[msk, :]\n",
    "    test = dataset.loc[~msk, :]\n",
    "    return train, test\n",
    "\n",
    "def make_sequence(df, selected_features, seq_num = 28, gap = 4):\n",
    "    dataset = pd.DataFrame(np.ones((int((df.shape[0]-seq_num)/gap), seq_num*len(selected_features)+1))*2 )\n",
    "    index = 0\n",
    "    state = False\n",
    "    last_contact_indexes = df.loc[df['label']==1,'index'].values\n",
    "    last_contact_indexes = last_contact_indexes[last_contact_indexes.shape[0]-1]\n",
    "\n",
    "    for i in range(0, last_contact_indexes, gap):\n",
    "        if state: \n",
    "            window =df[selected_features][i:i+seq_num]\n",
    "            dataset.iloc[index,0] = df['label'][i+seq_num]\n",
    "            dataset.iloc[index, 1:len(dataset.columns)] = np.hstack(window.to_numpy())\n",
    "            index += 1\n",
    "        else:\n",
    "            if df['label'][i+seq_num] == 1:\n",
    "                state = 1\n",
    "    dataset = dataset.drop(index=dataset.loc[dataset[0]==2,0].index)\n",
    "    return dataset\n",
    "\n",
    "def make_sequence_full_time(df, selected_features, seq_num = 28, gap = 4):\n",
    "    dataset = pd.DataFrame(np.ones((int((df.shape[0]-seq_num)), seq_num*len(selected_features)+1))*2 )\n",
    "    index = 0\n",
    "    last_contact_indexes = df.loc[df['label']==1,'index'].values\n",
    "    last_contact_indexes = last_contact_indexes[last_contact_indexes.shape[0]-1]\n",
    "\n",
    "    for i in range(0, df.shape[0]-seq_num-1, gap):\n",
    "\n",
    "        window =df[selected_features][i:i+seq_num]\n",
    "        dataset.iloc[index,0] = df['label'][i+seq_num]\n",
    "        dataset.iloc[index, 1:len(dataset.columns)] = np.hstack(window.to_numpy())\n",
    "        index += 1\n",
    "\n",
    "    dataset = dataset.drop(index=dataset.loc[dataset[0]==2,0].index)\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a dataset with a set of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(saveDatesetPath)\n",
    "train_split_rate = 0.7\n",
    "\n",
    "selected_features = tau + tau_ext + e + de\n",
    "seq_num = 28\n",
    "\n",
    "dict_label = {'a': 7, 'b':6, 'c':5, 'd':4, 'e':3, 'f':2, 'g':1}\n",
    "columns = range(seq_num*len(selected_features)+1)\n",
    "\n",
    "df_master_master_train = pd.DataFrame(columns=columns)\n",
    "df_master_master_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "    \n",
    "for i in os.listdir(labeledDataPath):\n",
    "    if len(i.split('.'))==1:\n",
    "\n",
    "        df_master_train = pd.DataFrame(columns=columns)\n",
    "        df_master_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "        submain_path = labeledDataPath+i+'/'\n",
    "        print(submain_path)\n",
    "        for tag_name in os.listdir(submain_path):\n",
    "            if len(tag_name.split('.'))==1:\n",
    "                file_path = submain_path+tag_name+'/labeled_data.csv'\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = make_sequence_full_time(df, selected_features,seq_num)\n",
    "                #labeling data\n",
    "                df[0] = df[0]*dict_label[tag_name[0]]\n",
    "                train, test = split_data(df, train_split_rate)\n",
    "                df_master_train = df_master_train.append(train, ignore_index=True)\n",
    "                df_master_test = df_master_test.append(test, ignore_index=True)\n",
    "                \n",
    "        df_master_train.to_pickle(saveDatesetPath+i+'_train.pkl')\n",
    "        df_master_test.to_pickle(saveDatesetPath+i+'_test.pkl')\n",
    "        \n",
    "    df_master_master_train = df_master_master_train.append(df_master_train, ignore_index=True)\n",
    "    df_master_master_test = df_master_master_test.append(df_master_test, ignore_index=True)\n",
    "\n",
    "df_master_master_train.to_pickle(saveDatesetPath+'dataset_train.pkl')\n",
    "df_master_master_test.to_pickle(saveDatesetPath+'dataset_test.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Print Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of all samples: ', df_master_master_train.shape[0]+df_master_master_test.shape[0])\n",
    "print('number of noncontact samples: ', df_master_master_train[0][df_master_master_train[0]==0].shape[0] +  df_master_master_test[0][df_master_master_test[0]==0].shape[0] )\n",
    "print('number of contact samples: ', df_master_master_train[0][df_master_master_train[0]!=0].shape[0] +  df_master_master_test[0][df_master_master_test[0]!=0].shape[0] )\n",
    "print('Link1 samples: ', df_master_master_train[0][df_master_master_train[0]==1].shape[0] +  df_master_master_test[0][df_master_master_test[0]==1].shape[0] )\n",
    "print('Link2 samples: ', df_master_master_train[0][df_master_master_train[0]==2].shape[0] +  df_master_master_test[0][df_master_master_test[0]==2].shape[0] )\n",
    "print('Link3 samples: ', df_master_master_train[0][df_master_master_train[0]==3].shape[0] +  df_master_master_test[0][df_master_master_test[0]==3].shape[0] )\n",
    "print('Link4 samples: ', df_master_master_train[0][df_master_master_train[0]==4].shape[0] +  df_master_master_test[0][df_master_master_test[0]==4].shape[0] )\n",
    "print('Link5 samples: ', df_master_master_train[0][df_master_master_train[0]==5].shape[0] +  df_master_master_test[0][df_master_master_test[0]==5].shape[0] )\n",
    "print('Link6 samples: ', df_master_master_train[0][df_master_master_train[0]==6].shape[0] +  df_master_master_test[0][df_master_master_test[0]==6].shape[0] )\n",
    "print('Link7 samples: ', df_master_master_train[0][df_master_master_train[0]==7].shape[0] +  df_master_master_test[0][df_master_master_test[0]==7].shape[0] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
